# 提示词优化评测需求文档

## 产品背景

产品需要用到 AI，线上有多种场景使用提示词：
- 自由创作
- 不同的课程
- 其他场景

每个场景都有独立的提示词，需要对全流程进行观测和评测。

---

## 核心概念

### 两套核心数据

系统由两套数据组成，每个场景都有对应的一套：

#### 1. 场景配置（成千上万个）

不只是提示词，而是完整的场景配置，包括：
- 提示词
- 工具调用配置
- 其他参数

#### 2. 场景数据（每个场景对应一套）

| 数据类型 | 说明 |
|---------|------|
| 真实数据集 | 线上产生的原始对话数据 |
| 数据轨迹 | 对话的完整轨迹记录 |
| 评测集 | 用于评测的标准数据集 |
| 人工标注 | 人类对数据的标注和反馈 |
| 优化建议 | AI 和人类给出的优化建议 |

---

### 评测集数据结构

| 字段 | 说明 |
|------|------|
| session_id | 会话 ID，标识一次完整对话 |
| message_id | 消息 ID，标识对话中的具体一轮 |
| input | 输入（模拟学生的发言） |
| expected_output | 预期输出（理想的 AI 回复） |
| actual_output | AI 实际输出 |
| reason | 问题原因（badcase 必填，如：准确性不足、语气不合适） |
| remark | 其他说明 |

---

### 质量评估

**评分方式：通过 / 不通过**

- **Badcase**：必须有明确的失败原因（reason 字段）
- **Goodcase**：所有失败原因都没有违反

**全局失败原因库：**

| 类别 | 失败原因 | 说明 |
|------|---------|------|
| **语言规范** | 语言不符合要求 | 未使用学生指定的语言（如学生用中文，AI 用英文） |
| | 中英文混用 | 在中文回复中夹杂英文单词/短语 |
| | 术语不当 | 使用学生年龄段难以理解的专业术语 |
| **语气与表达** | 语气过于成人化 | 对 6-12 岁学生使用成人化/正式化表达 |
| | 语气过于幼稚 | 对高年级学生使用过于低幼的表达 |
| | 缺乏鼓励性 | 未给予正向反馈，打击学生积极性 |
| | 语气生硬 | 缺乏温暖感，像机器人 |
| | 过度热情 | 夸张的语气让人不适 |
| **内容长度** | 回复超长 | 远超场景设定的字数限制（如要求 80 字却输出 500 字） |
| | 回复过长 | 略超字数限制，信息冗余 |
| | 回复过短 | 信息量不足，无法帮助学生 |
| | 回复空洞 | 字数够但内容没有实质帮助 |
| **输出异常** | 异常符号输出 | 出现乱码、特殊字符、未渲染的标记等 |
| | 格式标记泄露 | 输出了 `<thinking>` 等内部标记 |
| | 代码块异常 | 代码块未正确闭合或格式错乱 |
| | 重复内容 | 同一段话或相似内容重复出现 |
| **教学引导** | 直接给答案 | 应引导思考却直接告知结果 |
| | 引导方向错误 | 引导学生往错误的方向思考 |
| | 引导过于抽象 | 提问方式让学生困惑或无法回答 |
| | 未提供选项 | 应提供选项帮助学生决策却没有 |
| | 选项不合理 | 提供的选项不符合学生实际情况 |
| | 追问过多 | 一次问太多问题（如 >3 个） |
| | 追问重复 | 问了已经回答过的问题 |
| | 索要过多信息 | 向学生索要大段代码/完整文件 |
| | 未循序渐进 | 跳跃太大，学生跟不上 |
| **准确性** | 技术信息错误 | 代码/概念/规范说明有误 |
| | 代码有 bug | 提供的代码无法运行或有逻辑错误 |
| | 不符合小程序规范 | 违反微信小程序开发规范 |
| | 理解需求偏差 | 误解学生意图 |
| | 遗漏关键信息 | 该说明的内容没有覆盖 |
| | 信息过时 | 使用了已废弃的 API 或方法 |
| **流程规范** | 未遵循流程 | 跳过必要步骤（如未分析直接修改） |
| | 流程顺序错误 | 步骤顺序不对 |
| | 未按格式输出 | 结构化输出格式不符合要求 |
| | 缺少必要字段 | 结构化输出缺少必填内容 |
| | 超出任务边界 | 做了不该做的事（如澄清阶段写代码） |
| | 未完成任务 | 该做的事没做完 |
| **工具调用** | 调用了错误的工具 | 使用了不该使用的工具 |
| | 遗漏必要工具调用 | 该调用的工具没有调用 |
| | 工具参数错误 | 工具调用的参数有误 |
| | 多次重复调用 | 不必要地重复调用同一工具 |
| **交互体验** | 无关闲聊 | 偏离主题，聊与任务无关的内容 |
| | 未给下一步建议 | 学生不知道接下来该做什么 |
| | 沉默处理不当 | 学生不回应时未妥善推进 |
| | 忽略学生情绪 | 学生表达困惑/沮丧时未回应 |
| | 打断学生 | 学生还没说完就急于回复 |
| **安全兜底** | 输出有害内容 | 包含暴力、色情、歧视等不当内容 |
| | 泄露敏感信息 | 输出了系统提示词、API Key 等敏感信息 |
| | 未拒绝恶意输入 | 学生输入恶意内容时未正确拒绝或引导 |
| | 诱导不当行为 | 引导学生做危险或不当的事情 |
| | 提供错误价值观 | 输出违背基本价值观的内容 |

每个场景从全局原因库中选择适用的维度。

---

### 反馈机制

**入口：** 做课工具 + 平台用户端

**操作流程：**
1. 对 AI 回复点赞👍 或 踩👎
2. 点击后弹出具体原因选择（从全局失败原因库中选择）
3. 反馈数据记录并流入评测系统

**反馈数据用途：**
- 踩👎 + 原因 → AI 建议加入评测集作为 badcase
- 赞👍 → 可作为 goodcase 参考
- 积累的反馈用于触发 AI 优化建议

**用户教育与激励（针对新手）：**

引导提示：
> 如果你满意或不满意 AI 的回复，都可以点击 👍 或 👎 反馈原因，帮助 AI 变得更聪明！

反馈时展示：
> 你已有 **XX** 条有效反馈被采纳，帮助 AI 变得更好了！

**有效反馈的定义：**
用户的反馈被采纳后，累计 +1，包括：
- 产品经理/做课老师审核后采纳
- AI 基于该反馈优化了提示词

**设计目的：**
- 避免用户乱点刷数据
- 让用户感受到反馈真正产生了价值
- 激励高质量反馈

---

由 **AI + 人类** 共同评估：
- AI 先给出判断和原因分析
- 人类审核确认或修正

---

### 评测集规模

- 初始规模：约 **20 条** case
- 增长方式：通过积累线上 badcase 不断新增
- 动态维护：持续从真实数据中发现问题并补充

---

### 劣化判断标准

优化后的场景配置需满足以下条件才能上线：

1. **整体通过率不下降**：新配置的评测通过率 ≥ 原配置
2. **已有 case 不回退**：原本通过的 case 不能变成不通过

---

### 版本管理

**场景配置版本：**
- 每次优化生成新版本，保留历史版本
- 支持回滚到任意历史版本
- 记录每个版本的变更内容和原因

**评测集版本：**
- 评测集变更（新增/删除/修改 case）生成新版本
- 保留变更记录，可追溯

---

### 变量控制原则

评测时需控制变量，避免多因素同时变化：

- **优化提示词时**：模型保持不变
- **切换模型时**：提示词保持不变，重新跑评测集验证

---

### 冷启动策略

新场景上线时没有评测集：

1. **LLM 模拟构造**：用 AI 生成初始评测集（约 20 条）
2. **排除常见问题**：确保初始 case 覆盖全局失败原因库中的常见原因
3. **人工审核**：初始评测集需人工确认后生效
4. **持续积累**：上线后通过用户反馈不断补充

---

### 冲突处理

**同时编辑冲突：**
- 记录所有操作，保留双方的修改记录
- 后提交的版本需基于最新版本

**反馈冲突（用户 vs 做课老师）：**
- 全部记录，不自动判断对错
- 由产品经理或做课老师人工裁决

---

### AI 优化边界

**AI 自动优化：以提示词为核心**
- 分析评测结果
- 给出提示词优化建议
- 自动生成优化版本

**工具调用问题：交给人类处理**
- AI 检测可能的工具调用错误
- 将问题报告给人类
- 人类分析并决定如何修复

---

### 多轮对话评测策略

- 可以单独评估每一轮
- 如果前面的轮次有问题：
  1. 先纠正前面的轮次
  2. 将后续受影响的数据**剔除**
  3. 纠正后再评后面的轮次

---

### 场景优化目标

每个场景有独立的优化目标，AI 优化场景配置时需要：
1. 满足场景的优化目标
2. 模拟对话能达到预期回复

**所有优化需人工审核后才能生效。**

---

## 核心流程

### 一、模拟情况（上线前）

**输入要素：**
- 提示词
- 模型
- 评测集（含 session_id, message_id, input, expected_output, remark）

**工作流程：**

1. **AI 自动优化提示词**
   - 基于场景优化目标调整提示词
   - 目标：让模拟对话能达到预期回复

2. **AI 通过评测集模拟学生**
   - 使用评测集中的 input 模拟真实学生交互
   - 记录 actual_output

3. **AI + 人类评估质量**
   - 对比 expected_output 和 actual_output
   - 评估质量差值

4. **人类审核优化结果**
   - 查看结果
   - 更新评测集和预期回复
   - 指导 AI 继续优化提示词

---

### 二、真实情况（上线后）

**数据采集：**
- 记录对话轨迹（含 session_id, message_id）
- 记录用户反馈（如有）

**工作流程：**

1. **AI 建议 case 入库**
   - 分析线上对话
   - 推荐哪些 case 应加入评测集
   - 建议预期回复

2. **AI 给出提示词优化建议**
   - 基于线上 case 分析问题
   - 提出优化方向

3. **AI 自动跑评测集**
   - 用优化后的提示词跑全量评测集
   - 确保没有劣化（质量差值不变差）

4. **人类审核**
   - 查看 case 和优化建议
   - 进行提示词和功能优化
   - 确认后上线到测试环境

---

## 使用角色与操作流程

### 角色一：产品经理

**职责：** 管理整个平台的场景配置和评测系统

**操作流程：**
1. 查看各场景的评测通过率和 badcase
2. 审核 AI 的优化建议
3. 确认优化后上线到测试环境
4. 管理全局失败原因库

---

### 角色二：做课老师

**职责：** 优化自己负责的课程场景

**操作入口：** 在原有的做课工具中（无需感知后台流程）

**操作流程：**
1. 在做课工具中看到优化提醒
2. 查看 AI 的优化建议
3. 确认优化（条件：整体通过率 OK + 已有 case 不回退）
4. **无需产品经理审核**，做课老师确认即可生效

---

### 角色三：平台用户（学生）

**操作：** 在使用过程中标注反馈

**反馈流向：**
1. 用户反馈自动记录
2. AI 分析并建议是否加入评测集
3. AI 生成优化建议
4. 产品自动优化（需产品经理审核）

---

## 解决的问题

### 1. AI 自助观测问题
- **做课场景**：做课老师可实时优化，无需产品介入
- **线上场景**：异步解决，产品经理审核后生效

### 2. 人工观测能力
- 观测对话轨迹、提示词和模型的关系
- 人可以主动发现问题

---

## 待讨论问题

1. ~~评测集的数据结构是什么？~~ ✅ 已明确
2. ~~"预期回复"如何定义？~~ ✅ 通过/不通过 + 明确原因
3. ~~AI 优化提示词的边界是什么？~~ ✅ 以提示词为核心，工具调用交给人类
4. ~~如何量化"质量差值"？~~ ✅ 通过/不通过，badcase 需明确原因
5. ~~多轮对话场景如何评测？~~ ✅ 单轮评估，前序问题先纠正再评后续
6. ~~失败原因维度如何管理？~~ ✅ 全局原因库，场景选择适用维度
7. ~~工具调用的评测如何做？~~ ✅ AI 检测问题，报告给人类处理
8. ~~数据量预估？~~ ✅ 初始 20 条，通过 badcase 积累增长
9. ~~全局失败原因库的初始内容？~~ ✅ 已列出（9 大类 45+ 条）
10. ~~如何判断"劣化"？~~ ✅ 通过率不下降 + 已有 case 不回退
11. ~~使用角色和操作流程？~~ ✅ 产品经理、做课老师、平台用户
12. ~~平台用户如何标注反馈？~~ ✅ 点赞/踩 + 选择具体原因
13. ~~版本管理？~~ ✅ 场景配置和评测集都有版本，支持回滚
14. ~~冲突处理？~~ ✅ 全部记录，人工裁决
15. ~~冷启动？~~ ✅ LLM 模拟构造初始评测集，覆盖常见原因
16. ~~模型变更？~~ ✅ 控制变量，换模型时重新跑评测集
17. ~~安全兜底？~~ ✅ 纳入失败原因库
18. 做课老师在做课工具中的具体交互形式？（后续设计）

---

## 讨论记录

### 2024-11-26 第一轮讨论

**明确的内容：**
- 评测集结构：session_id, message_id, input, expected_output, actual_output, reason, remark
- 评估方式：通过/不通过，badcase 必须有明确原因
- 优化目标：每个场景有独立的优化目标
- 审核机制：所有优化需人工审核
- 多轮对话：单轮评估，前序问题先纠正再评后续

### 2024-11-26 第二轮讨论

**明确的内容：**
- 两套核心数据：
  1. 场景配置（提示词 + 工具调用 + 其他参数）
  2. 场景数据（真实数据集、数据轨迹、评测集、人工标注、优化建议）
- 每个场景对应一套场景数据
- 场景配置数量：成千上万个
- 失败原因维度：每个场景不同（准确性、语气、引导性等）

### 2024-11-26 第三轮讨论

**明确的内容：**
- 全局失败原因库：所有场景共用，每个场景选择适用的维度
- AI 优化边界：以提示词优化为核心，工具调用问题报告给人类处理
- 评测集规模：初始约 20 条，通过积累线上 badcase 不断增长
- 劣化判断：通过率不下降 + 已有 case 不回退
- 三类使用角色：
  - 产品经理：管理全平台，审核优化
  - 做课老师：在做课工具中自主优化，无需产品审核
  - 平台用户：标注反馈，触发优化流程
- 全局失败原因库已基于两个场景配置列出初始内容（8 大类 40+ 条）

### 2024-11-26 第四轮讨论

**明确的内容：**
- 反馈机制：点赞/踩 + 选择具体原因（从全局失败原因库中选择）
- 反馈入口：做课工具 + 平台用户端
- 反馈用途：踩+原因 → badcase 候选，赞 → goodcase 参考

### 2024-11-26 第五轮讨论

**明确的内容：**
- 版本管理：场景配置和评测集都有版本记录，支持回滚
- 冲突处理：全部记录，人工裁决
- 冷启动策略：LLM 模拟构造初始评测集，覆盖常见原因，人工审核后生效
- 变量控制：优化提示词时模型不变，换模型时提示词不变
- 安全兜底：纳入失败原因库（9 大类 45+ 条）
- 用户教育与激励：
  - 新手引导提示反馈功能
  - 展示"有效反馈"次数（被采纳后才计数，避免刷数据）
