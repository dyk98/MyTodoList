# 教育自由创作并发能力优化分析

## 背景

原有自由创作场景存在高频高耗问题，单次请求平均耗时 2 分钟，严重限制了系统并发能力。通过场景拆分和模型替换两项优化措施，显著提升了系统吞吐量。

## 优化前状态

| 指标 | 数值 |
|------|------|
| 模型 | qwen3coder |
| TPM (Tokens Per Minute) | 12,000 |
| 单次请求平均耗时 | 2 分钟 |
| 场景类型 | 高频高耗（未拆分） |

### 原始并发能力分析

- 单次请求占用时间：2 分钟
- 每分钟可处理请求数：约 0.5 次
- 并发瓶颈：长耗时请求持续占用 TPM 配额

## 优化措施

### 优化一：场景拆分

将原本的高频高耗场景拆分为两类：

| 场景类型 | 平均耗时 | 调用频率 |
|---------|---------|---------|
| 高频低耗场景 | 30 秒 | 高 |
| 低频高耗场景 | 2 分钟 | 低 |

**耗时优化比**：2 分钟 → 30 秒 = **4 倍提升**

### 优化二：模型替换

| 指标 | 优化前 | 优化后 |
|------|-------|-------|
| 模型 | qwen3coder | MiniMax-M2 |
| 参数量 | 480B | 230B |
| TPM | 12,000 | 13,500 |

**TPM 优化比**：13,500 / 12,000 = **1.125 倍提升**

### 优化三：实例扩容（资源不变）

由于模型参数量从 480B 降至 230B，在相同计算资源下可部署更多推理实例：

```
实例扩容比 = 原参数量 / 新参数量
          = 480B / 230B
          ≈ 2.09 倍
```

**实例扩容提升**：相同资源可部署约 **2.09 倍** 实例数

## 综合效果计算

### 高频场景并发能力提升

```
综合提升倍数 = 场景拆分提升 × TPM 提升 × 实例扩容提升
            = 4 × 1.125 × 2.09
            ≈ 9.4 倍
```

### 详细对比

| 指标 | 优化前 | 优化后（高频场景） | 提升倍数 |
|------|-------|------------------|---------|
| 单次耗时 | 120 秒 | 30 秒 | 4× |
| TPM 限制 | 12,000 | 13,500 | 1.125× |
| 可部署实例数 | 1× | 2.09× | 2.09× |
| 单实例每分钟可处理请求数 | ~0.5 | ~2.25 | 4.5× |
| **系统总吞吐量** | 基准 | - | **9.4×** |
| 用户平均等待时间 | 2 分钟 | 30 秒 | 减少 75% |

### 系统整体吞吐量

假设场景调用分布：
- 高频场景占比：80%
- 低频场景占比：20%

低频场景提升（仅受 TPM 和实例扩容影响）：
```
低频场景提升 = TPM 提升 × 实例扩容提升
            = 1.125 × 2.09
            ≈ 2.35 倍
```

**加权平均提升**：
```
整体提升 = 0.8 × 9.4 + 0.2 × 2.35
        = 7.52 + 0.47
        ≈ 8 倍
```

## 实测数据佐证

### 测试环境

- **测试时间**：2025-12-01
- **测试模型**：minimax_m2_code
- **测试接口**：astra.woa.com/ai-coding/v1
- **测试模式**：阶梯并发测试（ladder）

### 阶梯并发测试结果

| 并发数 | QPM | TPM | 平均延迟 | 错误率 | TPS |
|-------|-----|-----|---------|-------|-----|
| 1 | 10.48 | 5,696 | 5.73s | 0% | 89.41 |
| 2 | 16.03 | 8,693 | 5.63s | 0% | 347.69 |
| 4 | 24.79 | 13,521 | 7.26s | 0% | 95.30 |
| 8 | 23.33 | 12,689 | 7.01s | 0% | 123.14 |
| 16 | 23.29 | 12,649 | 7.01s | 0% | 74.66 |

### 数据分析

1. **最优并发点**：并发数 4 时达到 TPM 峰值 **13,521**，接近 TPM 上限 13,500
2. **并发扩展性**：
   - 并发 1→2：QPM 提升 53%（10.48 → 16.03）
   - 并发 2→4：QPM 提升 55%（16.03 → 24.79）
   - 并发 4→8：QPM 略降（已触及 TPM 上限）
3. **延迟表现**：
   - 低并发（1-2）：平均延迟稳定在 ~5.7s
   - 高并发（4+）：延迟略升至 ~7s，但仍在可接受范围
4. **稳定性**：全部测试 0% 错误率，系统表现稳定

### 实测验证结论

| 指标 | 理论值 | 实测值 | 验证结果 |
|-----|-------|-------|---------|
| TPM 上限 | 13,500 | 13,521 | 符合预期 |
| 最大 QPM | - | 24.79 | 基准数据 |
| 最优并发数 | - | 4 | 实测确认 |

实测数据证明：
- 新模型 TPM 配额（13,500）可被充分利用
- 并发数 4 时可达最优吞吐量
- 系统在高并发下保持稳定（0% 错误率）

---

## 总结

| 优化项 | 提升效果 |
|-------|---------|
| 场景拆分（高频场景耗时优化） | 4 倍 |
| 模型替换（TPM 提升） | 1.125 倍 |
| 实例扩容（参数量 480B→230B） | 2.09 倍 |
| **高频场景综合提升** | **9.4 倍** |
| **系统整体提升（加权）** | **约 8 倍** |

### 关键收益

1. **用户体验提升**：高频场景响应时间从 2 分钟降至 30 秒，减少 75%
2. **系统吞吐量提升**：相同资源下可服务用户数提升约 9 倍
3. **资源利用率优化**：小参数模型（230B vs 480B）可部署 2 倍实例，同时获得更高 TPM 配额
4. **弹性扩展能力**：短请求更易于负载均衡和故障恢复
