# 场景配置自优化系统产品方案

## 一、背景与目标

### 我们面临的问题

我们的产品有成千上万个 AI 场景配置，每个场景都有独立的提示词、工具配置和参数。目前这些配置的优化主要依赖人工：

- **发现问题靠运气**：只有用户反馈或做课老师偶然发现，才知道哪里有问题
- **优化效率低**：每次优化都需要人工分析、修改、测试，周期长
- **质量难保证**：改了一个问题可能引入新问题，缺乏系统性的回归验证
- **经验难沉淀**：优化经验分散在个人脑中，无法规模化复用

### 我们要做什么

构建一个 **AI 自优化系统**，让场景配置能够：

```
自提问 → 自执行 → 自归因 → 自优化
   ↑                          ↓
   └──────── 循环 ─────────────┘
```

同时支持人工在任意环节介入，形成 **Human-in-the-loop** 的协作模式。

### 核心价值

| 价值点 | 说明 |
|--------|------|
| **降低人力成本** | AI 自动发现问题、生成优化方案，人工只需审核确认 |
| **提升迭代效率** | 从"周级别"优化缩短到"小时级别" |
| **保障质量下限** | 自动化回归测试，确保改动不引入新问题 |
| **沉淀优化经验** | 每次优化的原因、改动都有记录，可追溯可复用 |

---

## 二、整体思路

### 2.1 两套核心数据

系统围绕两套数据运转：

**场景配置**（我们要优化的对象）
- 提示词（personality、goals、knowledge）
- 模拟对话历史（userContext）
- 工具配置（后续支持）

**评测数据**（用来验证优化效果）
- 评测集：模拟用户输入 + 预期行为
- 执行记录：AI 实际的回复
- 评判结果：通过/不通过 + 失败原因

### 2.2 闭环流程

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   ┌─────────┐      ┌─────────┐      ┌─────────┐            │
│   │  评测集  │ ──▶ │  执行   │ ──▶ │  评判   │            │
│   │         │      │  场景   │      │  质量   │            │
│   └─────────┘      └─────────┘      └─────────┘            │
│        ▲                                  │                 │
│        │                                  ▼                 │
│   ┌─────────┐      ┌─────────┐      ┌─────────┐            │
│   │  生成   │ ◀── │  优化   │ ◀── │  归因   │            │
│   │ 新配置  │      │  建议   │      │  分析   │            │
│   └─────────┘      └─────────┘      └─────────┘            │
│                                                             │
│                    人工可在任意环节介入                       │
└─────────────────────────────────────────────────────────────┘
```

---

## 三、核心能力详解

### 3.1 自提问：AI 生成评测集

#### 问题

评测集是验证场景质量的基础，但人工编写评测集成本高、覆盖面有限。

#### 解决方案

**方式一：从零生成**

AI 扮演不同角色（如 6 岁初学者、12 岁进阶学生、故意捣乱的学生），模拟真实用户与场景交互，自动生成评测用例。

```
输入：
- 场景配置（告诉 AI 这个场景是做什么的）
- 角色配置（告诉 AI 要扮演什么样的用户）
- 优化目标（可选，如"希望回复更有亲和力"）

输出：
- 多轮对话的评测用例
- 每轮的预期行为描述（语义化，如"鼓励学生，语气轻松"）
```

**方式二：基于种子集二次创作**

人工只需编写少量高质量的种子用例（5-10 条），AI 基于这些种子扩展出更多用例。

支持的扩展策略：
- **变体生成**：保持意图不变，改变措辞表达
- **边界情况**：生成极端输入、模糊表达等
- **角色改写**：用不同用户角色改写同一对话
- **难度梯度**：生成更简单或更复杂的版本
- **对抗测试**：生成可能导致 AI 失败的输入

#### 预期效果

- 人工编写 10 条种子，AI 可扩展到 50-100 条
- 覆盖正常流程、边界情况、对抗场景
- 大幅降低评测集构建成本

---

### 3.2 自执行：自动运行评测

#### 问题

手动测试场景效果耗时耗力，且难以保证每次都测试完整。

#### 解决方案

系统自动执行评测集中的所有用例：

1. 按评测集中的输入，模拟用户与场景交互
2. 收集 AI 的实际回复
3. 支持单场景评测，也支持场景链（多个场景串联）评测
4. 场景链中任一环节失败，整体判定为失败

#### 场景链评测示例

```
评测目标：devtoolsCD（需求澄清 → 设计扩写）

执行流程：
1. 用户输入 → 澄清场景执行 → 收集澄清结果
2. 澄清结果 → 设计场景执行 → 收集设计结果
3. 评判每个环节的质量

如果澄清场景失败 → 整个评测判定失败，后续不再执行
```

---

### 3.3 自归因：AI 分析问题根因

#### 问题

知道"哪里错了"不够，还要知道"为什么错"才能有效优化。传统做法依赖人工分析，效率低且主观性强。

#### 解决方案

AI 自动进行深度归因分析，支持三种场景：

**场景一：基于评测结果的批量归因**

分析评测中所有失败的用例，找出共性问题。

```
输入：一次评测运行的结果（含多个失败用例）

分析维度：
- 失败原因分布：哪类问题最多？
- 对话流程分析：失败通常发生在什么阶段？
- 根因定位：问题出在配置的哪个字段？

输出：
- 问题出在 personality 字段，角色定位过于正式
- 证据：case_001、case_003 中回复语气生硬
- 建议：增加"用温暖友好的语气与学生交流"
```

**场景二：基于单个对话的归因（支持人工反馈）**

用户对某轮回复点了踩，系统分析为什么 AI 会这样回复。

```
输入：
- 完整对话轨迹（不只是出问题的那一轮）
- 人工反馈："这里回复太生硬了"

分析维度：
- 对话流程是否流畅？
- 关键转折点在哪里？
- AI 是否正确理解了上下文？

输出：
- 第 3 轮是关键转折点，AI 没有识别到学生的困惑
- 根因：goals 中缺少"关注学生情绪变化"的要求
```

**场景三：基于多个对话的整体优化**

不只是修复 badcase，而是整体提升对话质量。

```
输入：
- 多个对话轨迹（可以都是"还行"但不够好的）
- 优化目标："提升对话的引导效果"

分析维度：
- 多个对话的共性模式
- 与优化目标的差距
- 系统性改进方向

输出：
- 共性问题：引导过于抽象，学生难以跟上
- 建议：在 goals 中增加"用具体例子引导，而非抽象提问"
```

---

### 3.4 自优化：AI 生成新配置

#### 问题

知道问题在哪后，人工修改配置仍然需要时间，且修改质量参差不齐。

#### 解决方案

AI 基于归因结果，自动生成优化后的场景配置。

```
输入：
- 当前配置
- 归因分析结果
- 场景优化目标

输出：
- 新版本配置（完整可用）
- 改了什么：personality 字段
- 为什么改：解决"语气生硬"问题
- 改动前后对比
```

#### 版本管理

每次优化生成新版本，命名规则：`{场景名}_opt_{日期}_{序号}`

例如：`educationSimpleModify_opt_251127_01`

支持：
- 查看所有历史版本
- 对比任意两个版本的差异
- 回滚到任意历史版本

---

### 3.5 循环优化：一键多轮迭代

#### 问题

单次优化可能只解决部分问题，需要多轮迭代才能达到理想效果。手动触发每一轮太繁琐。

#### 解决方案

支持设置循环次数，系统自动执行多轮优化：

```
配置：
- 最大循环次数：4 轮
- 提前停止条件：通过率达到 95%

执行过程：
┌────────────────────────────────────────┐
│  第 1 轮（baseline）                    │
│  评测 → 通过率 70%                      │
│  归因 → 发现"语气生硬"                  │
│  优化 → 生成 v1                        │
├────────────────────────────────────────┤
│  第 2 轮                               │
│  评测 v1 → 通过率 80%（+10%）           │
│  归因 → 发现"引导不足"                  │
│  优化 → 生成 v2                        │
├────────────────────────────────────────┤
│  第 3 轮                               │
│  评测 v2 → 通过率 90%（+10%）           │
│  归因 → 发现"回复过长"                  │
│  优化 → 生成 v3                        │
├────────────────────────────────────────┤
│  第 4 轮                               │
│  评测 v3 → 通过率 95%（+5%）            │
│  达到停止条件，结束                      │
└────────────────────────────────────────┘
```

#### 版本选择

循环结束后，系统展示所有版本的对比：

| 版本 | 通过率 | 相比基线 | 回退数 | 说明 |
|------|--------|---------|--------|------|
| baseline | 70% | - | - | 当前线上版本 |
| v1 | 80% | +10% | 0 | |
| v2 | 90% | +20% | 1 | 有 1 个 case 回退 |
| v3 | 95% | +25% | 0 | **系统推荐** |

人工可以：
- 选择系统推荐的版本（通过率最高且无回退）
- 选择更保守的版本（如 v1，改动小）
- 不选择任何版本（继续人工调整）

---

### 3.6 劣化检测：确保不引入新问题

#### 问题

优化场景时最怕"按下葫芦浮起瓢"——修好了一个问题，又引入了新问题。

#### 解决方案

每次优化后自动进行劣化检测：

**规则一：整体通过率不下降**
- 新配置的通过率必须 ≥ 原配置

**规则二：已有 case 不回退**
- 原本通过的 case 不能变成不通过
- 即使整体通过率提升，有回退也要警告

```
检测结果示例：

✅ 可以部署
- 通过率：70% → 85%（+15%）
- 新通过：case_003, case_007, case_012
- 回退：无

⚠️ 谨慎部署
- 通过率：70% → 75%（+5%）
- 新通过：case_003, case_007
- 回退：case_001（原本通过，现在失败）

❌ 不建议部署
- 通过率：70% → 65%（-5%）
- 整体质量下降
```

---

## 四、质量评估体系

### 4.1 评分方式

采用 **通过/不通过** 的二元评判，而非分数：

- **通过（Goodcase）**：没有触发任何失败原因
- **不通过（Badcase）**：触发了至少一个失败原因，必须明确标注是哪个

### 4.2 预期行为的定义

评测集中的预期行为支持三种形式：

**语义预期（推荐）**
- 描述 AI 应该怎么做，而非具体说什么
- 例如："鼓励学生，语气轻松，引导下一步"
- 灵活，允许 AI 有不同的表达方式

**精确预期**
- 提供完整的参考回复
- 适合有标准答案的场景
- 不灵活，但评判明确

**约束条件**
- 设置硬性规则
- 例如：回复不超过 100 字、必须包含"加油"、不能出现英文
- 可与语义预期组合使用

### 4.3 失败原因库

系统维护一个全局的失败原因库，每个场景选择适用的维度：

| 类别 | 失败原因示例 |
|------|-------------|
| 语言规范 | 语言不符合要求、中英文混用、术语不当 |
| 语气表达 | 语气过于成人化、语气生硬、缺乏鼓励 |
| 内容长度 | 回复超长、回复过短、回复空洞 |
| 输出异常 | 异常符号、格式泄露、重复内容 |
| 教学引导 | 直接给答案、引导方向错误、追问过多 |
| 准确性 | 技术信息错误、代码有 bug、理解偏差 |
| 流程规范 | 未遵循流程、超出任务边界、未完成任务 |
| 工具调用 | 调用错误工具、遗漏必要调用、参数错误 |
| 交互体验 | 无关闲聊、未给下一步建议、忽略情绪 |
| 安全兜底 | 有害内容、泄露敏感信息、错误价值观 |

---

## 五、使用场景

### 场景一：新场景冷启动

**背景**：新开发了一个场景，还没有评测数据

**流程**：
1. 配置角色（选择或创建适合该场景的用户角色）
2. AI 从零生成初始评测集（约 20 条）
3. 人工审核，修正不合理的用例
4. 运行评测，查看 baseline 通过率
5. 启动循环优化，提升通过率
6. 选择最佳版本部署

### 场景二：用户反馈驱动优化

**背景**：用户对某轮回复点了踩，反馈"回复太生硬"

**流程**：
1. 系统捕获完整对话轨迹和用户反馈
2. AI 分析整个对话，找出问题根因
3. AI 生成优化建议和新配置
4. 在评测集中补充这个 case
5. 运行评测，确保优化有效且无回退
6. 人工确认后部署

### 场景三：定期质量巡检

**背景**：希望定期检查线上场景质量

**流程**：
1. 配置定时任务（如每天凌晨 2 点）
2. 系统自动运行所有场景的评测
3. 发现通过率下降或新 badcase 时告警
4. AI 自动归因并生成优化建议
5. 人工审核后决定是否部署

### 场景四：整体对话质量提升

**背景**：场景没有明显 badcase，但整体对话体验一般

**流程**：
1. 收集多个真实对话轨迹
2. 设定优化目标（如"提升引导效果"）
3. AI 分析共性问题，给出系统性优化建议
4. 生成新配置并评测
5. 对比优化前后的对话效果
6. 人工确认后部署

---

## 六、Human-in-the-loop 设计

系统在每个关键环节都支持人工介入：

| 环节 | AI 做什么 | 人工做什么 |
|------|----------|-----------|
| 评测集生成 | 自动生成用例 | 审核、修正、补充边界情况 |
| 评测执行 | 自动运行、收集结果 | 查看异常、手动重跑 |
| 质量评判 | 自动判断通过/失败 | 审核评判结果、修正误判 |
| 归因分析 | 分析根因、给出建议 | 确认分析是否准确 |
| 优化生成 | 生成新配置 | 审核改动、调整细节 |
| 版本部署 | 推荐最佳版本 | 最终选择、确认部署 |

**原则**：AI 做繁重的分析和生成工作，人工做最终的决策和把关。

---

## 七、与现有系统的关系

### 输入
- 现有的场景配置（sceneList.json）
- 用户反馈数据（点赞/踩 + 原因）
- 线上对话轨迹

### 输出
- 优化后的场景配置（新版本）
- 评测报告
- 归因分析报告

### 不改变
- 场景的运行时逻辑（Driver）
- 工具调用机制
- 模型配置管理

---

## 八、预期收益

### 效率提升

| 指标 | 现状 | 目标 |
|------|------|------|
| 单场景优化周期 | 1-2 周 | 1-2 天 |
| 评测集构建成本 | 人工编写 | 人工审核 AI 生成 |
| 问题发现时间 | 用户反馈后 | 定时巡检主动发现 |

### 质量保障

- 每次优化都有评测验证
- 劣化检测确保不引入新问题
- 优化历史可追溯

### 经验沉淀

- 失败原因库持续积累
- 优化案例可复用
- 角色配置可跨场景使用

---

## 九、实施计划

### Phase 1：核心闭环
- 评测集管理（生成、二次创作、人工编辑）
- 评测执行引擎
- 归因分析（支持对话轨迹）
- 优化生成
- 循环优化
- 劣化检测

### Phase 2：工具调用优化
- 支持 tools 字段的优化
- 工具调用序列评测
- 工具参数正确性检查

### Phase 3：线上闭环
- 线上数据自动入库
- 用户反馈自动处理
- A/B 测试支持

---

## 十、总结

这个系统的核心理念是：

> **让 AI 优化 AI，人工把关决策**

通过自动化的评测、归因、优化循环，大幅降低场景配置的优化成本，同时通过劣化检测和人工审核确保质量。最终实现场景配置的持续改进，让每一次用户反馈都能转化为实际的优化，让每一个场景都能越用越好。
