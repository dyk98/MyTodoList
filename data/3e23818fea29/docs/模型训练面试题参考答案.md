# 模型训练算法实习生面试题参考答案

## 1. 讲讲 PPO 和 DPO 的区别，各自的优缺点是什么？GRPO 做了什么改进？GSPO 和 GDPO 又是什么

### PPO (Proximal Policy Optimization)

**核心思想**：通过限制策略更新幅度来稳定训练

**流程**：
1. 先训练一个 Reward Model (RM)
2. 用 RM 给 response 打分
3. 通过 RL 优化策略，使用 clip 机制限制更新幅度

**优点**：
- 训练稳定，通过 clip 防止策略突变
- 可以在线采样，持续改进
- 理论基础扎实

**缺点**：
- 需要额外训练 Reward Model
- 计算开销大（需要 4 个模型：policy、ref、reward、critic）
- 超参数敏感（clip ratio、KL 系数等）
- Reward Model 的质量直接影响最终效果

---

### DPO (Direct Preference Optimization)

**核心思想**：跳过显式的 RM 训练，直接从偏好数据优化策略

**数学推导**：将 RLHF 的目标函数重参数化，得到闭式解：
```
L_DPO = -E[log σ(β · (log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x)))]
```

**优点**：
- 简单，只需要 SFT 式的训练
- 计算高效（只需 2 个模型：policy、ref）
- 无需 RM，避免 reward hacking

**缺点**：
- Off-policy：使用固定的偏好数据，无法在线探索
- 对数据质量要求高
- 隐式 reward 可能不准确
- 缺乏对 reward 的显式建模，难以调试

---

### PPO vs DPO 对比

| 维度 | PPO | DPO |
|------|-----|-----|
| 模型数量 | 4 个（policy、ref、reward、critic） | 2 个（policy、ref） |
| 训练方式 | On-policy，在线采样 | Off-policy，离线数据 |
| 是否需要 RM | 是 | 否 |
| 计算成本 | 高 | 低 |
| 调参难度 | 高 | 低 |
| 探索能力 | 强 | 弱 |

---

### GRPO (Group Relative Policy Optimization)

**DeepSeek 提出的改进**，核心改动：

1. **去掉 Critic Model**：不再需要价值函数估计
2. **组内相对奖励**：对同一 prompt 采样多个 response，用组内排名计算优势函数
   ```
   A_i = (r_i - mean(r_group)) / std(r_group)
   ```
3. **更高效**：减少一个模型，降低显存和计算

**优点**：减少模型数量，训练更稳定

**适用场景**：DeepSeek-R1 等推理模型训练

---

### GSPO (Group Structured Preference Optimization)

**核心思想**：结合 DPO 和 Group 采样

- 对同一 prompt 采样多个 response
- 构建组内的偏好对（pairwise comparison）
- 使用 DPO loss 但基于组内结构化的偏好关系

---

### GDPO (Group Direct Preference Optimization)

**是 GRPO 和 DPO 的融合**：

- 使用 DPO 的 loss 形式
- 但采用 GRPO 的组采样策略
- 不需要显式的 RM，但利用组内对比信号

**关系总结**：
```
PPO → GRPO（去 Critic，组采样）
DPO → GSPO/GDPO（加组采样）
```

---

## 2. 什么是 Reward Hacking？怎么缓解？

### 什么是 Reward Hacking？

模型学会了「欺骗」Reward Model 获得高分，但实际输出质量并没有提升。

**常见表现**：
- 输出变长（RM 可能偏好长回答）
- 重复某些高分 pattern
- 使用讨好性语言（"Great question!"）
- 格式化输出（列表、代码块）即使不需要
- 回避问题但措辞圆滑

**根本原因**：RM 是真实人类偏好的不完美代理（proxy）

### 缓解方法

1. **KL 散度惩罚**：限制策略偏离 reference model 太远
   ```
   reward_final = reward - β * KL(π || π_ref)
   ```

2. **Reward Model Ensemble**：多个 RM 投票，减少单一 RM 的偏见

3. **定期更新 RM**：用新策略产生的数据重新训练 RM

4. **长度惩罚**：显式惩罚过长输出
   ```
   reward_final = reward - α * length
   ```

5. **Constrained RL**：设置硬约束，如最大长度、禁止特定 pattern

6. **Process Reward Model (PRM)**：奖励推理过程而非仅结果

7. **RLHF + Rule-based Reward**：结合规则约束

---

## 3. 怎么理解 on policy 和 off policy，会有什么区别

### On-Policy

**定义**：用当前策略 π 采样的数据来更新 π 本身

**例子**：PPO、REINFORCE

**特点**：
- 数据即采即用，用完丢弃
- 采样效率低（需要大量交互）
- 策略更新更稳定
- 可以持续探索新区域

### Off-Policy

**定义**：可以用其他策略（历史策略、专家策略）产生的数据来更新当前策略

**例子**：DPO、DQN

**特点**：
- 可以复用历史数据（Experience Replay）
- 采样效率高
- 但有 distribution shift 问题
- 需要 importance sampling 修正

### 核心区别

| 维度 | On-Policy | Off-Policy |
|------|-----------|------------|
| 数据来源 | 当前策略 | 任意策略 |
| 数据复用 | 不复用 | 可复用 |
| 采样效率 | 低 | 高 |
| 分布偏移 | 无 | 需要处理 |
| 典型算法 | PPO, A2C | DQN, DPO |

### 在 LLM 中的体现

- **PPO (On-policy)**：每次更新需要重新采样 response
- **DPO (Off-policy)**：使用预先收集的偏好数据，不需要在线采样

**DPO 的问题**：偏好数据可能来自旧策略，与当前策略分布不匹配，导致优化方向偏移

---

## 4. SFT 和预训练分别是什么，为了解决什么问题

### 预训练 (Pre-training)

**目标**：学习语言的通用表示和世界知识

**方式**：
- 自监督学习，预测下一个 token（GPT 风格）
- 或 Masked Language Model（BERT 风格）

**数据**：大规模无标注文本（网页、书籍、代码等）

**解决的问题**：
- 学习语法、语义、常识
- 获取世界知识
- 建立通用的语言理解能力

### SFT (Supervised Fine-Tuning)

**目标**：让模型学会「对话」和「遵循指令」

**方式**：在 (instruction, response) 对上做监督学习

**数据**：高质量的指令-回答对

**解决的问题**：
- 预训练模型只会「续写」，不会「回答」
- 让模型理解用户意图
- 对齐输出格式

### 训练流程关系

```
预训练 → SFT → RLHF
知识获取   指令遵循   偏好对齐
```

---

### 4a. RAG 和 SFT 的区别是什么，什么时候用 RAG 什么时候用 SFT

| 维度 | RAG | SFT |
|------|-----|-----|
| 本质 | 检索增强，外挂知识库 | 微调模型参数 |
| 知识存储 | 外部数据库 | 模型权重 |
| 更新成本 | 低（更新数据库） | 高（重新训练） |
| 实时性 | 高 | 低 |
| 幻觉问题 | 有溯源，可缓解 | 仍存在 |

**什么时候用 RAG**：
- 知识频繁更新（新闻、文档）
- 需要溯源和引用
- 私有数据不想训入模型
- 长尾知识（不值得训练）

**什么时候用 SFT**：
- 改变模型行为/风格
- 学习特定任务的格式
- 知识相对稳定
- 需要更快的推理速度（无检索延迟）

**最佳实践**：两者结合，SFT 教模型「如何使用检索结果」，RAG 提供实时知识

---

## 5. MHA GQA MQA 的区别和作用，什么情况需要 mask，mask 如何设计

### 三种注意力机制

**MHA (Multi-Head Attention)**
- 标准多头注意力
- 每个头有独立的 Q、K、V
- 参数量：3 × d_model × d_model
- KV Cache 大小：batch × n_heads × seq_len × d_head

**MQA (Multi-Query Attention)**
- 所有头共享同一组 K、V，只有 Q 是多头的
- 参数量减少约 2/3
- KV Cache 减少 n_heads 倍
- **缺点**：质量有一定下降

**GQA (Grouped-Query Attention)**
- MHA 和 MQA 的折中
- 将 heads 分组，每组共享 K、V
- 例如 8 heads 分 2 组，每组 4 heads 共享一套 KV
- **平衡**：质量接近 MHA，效率接近 MQA

**对比示意**：
```
MHA:  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
      K1 K2 K3 K4 K5 K6 K7 K8  (8套KV)

GQA:  Q1 Q2 Q3 Q4 | Q5 Q6 Q7 Q8
      K1 K1 K1 K1 | K2 K2 K2 K2  (2套KV)

MQA:  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
      K1 K1 K1 K1 K1 K1 K1 K1  (1套KV)
```

---

### Mask 的作用

**什么情况需要 Mask**：

1. **Causal Mask（因果掩码）**
   - 用于自回归生成
   - 防止当前 token 看到未来 token
   - GPT 系列必须使用

2. **Padding Mask**
   - 处理变长序列时，忽略 padding token
   - 防止 attention 分配到无意义位置

3. **Cross-attention Mask**
   - Encoder-Decoder 架构中
   - 控制 Decoder 能看到哪些 Encoder 位置

**Mask 设计**：

```python
# Causal Mask (下三角为1，上三角为0)
causal_mask = torch.tril(torch.ones(seq_len, seq_len))

# Padding Mask
padding_mask = (input_ids != pad_token_id).float()

# 组合使用
attention_mask = causal_mask * padding_mask
# 然后将 0 的位置填充 -inf
attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
```

**Bidirectional vs Causal**：
- BERT：无 causal mask，双向看
- GPT：有 causal mask，单向看
- Prefix LM（T5、GLM）：prefix 部分双向，生成部分单向

---

## 6. 讲讲你理解的 Agent 的发展，代码生成的 agent 系统要怎么设计，如果让你来训练，你会挑选哪些 query，怎么评估效果设计指标

### Agent 发展历程

**第一阶段：ReAct 范式（2022）**
- Reason + Act：思考-行动循环
- 简单的工具调用

**第二阶段：多 Agent 协作（2023）**
- AutoGPT、MetaGPT
- 角色分工，协作完成复杂任务

**第三阶段：Tree/Graph of Thoughts**
- 不是线性思考，而是树状搜索
- 可回溯、可并行探索

**第四阶段：自我进化 Agent（2024-现在）**
- Reflection：自我反思和修正
- 强化学习训练 Agent 行为
- DeepSeek-R1 式的长思考链

---

### 代码生成 Agent 系统设计

**核心模块**：

```
┌─────────────────────────────────────────────┐
│                 Orchestrator                 │
├─────────────────────────────────────────────┤
│  Planning  │  Coding  │  Testing  │ Review  │
├─────────────────────────────────────────────┤
│        Tool Layer (File, Shell, Git)        │
├─────────────────────────────────────────────┤
│           Memory (Short + Long)             │
└─────────────────────────────────────────────┘
```

1. **Planning Agent**：理解需求，拆解任务
2. **Coding Agent**：生成/修改代码
3. **Testing Agent**：写测试、运行测试
4. **Review Agent**：代码审查、错误修复

**关键设计点**：
- **迭代循环**：写代码 → 测试 → 修复 → 再测试
- **上下文管理**：代码库太大时，智能检索相关文件
- **工具设计**：文件读写、shell 执行、git 操作

---

### 训练数据选择（Query 挑选）

**高质量 Query 特征**：
- 真实用户场景（从 IDE 插件、GitHub Issue 收集）
- 难度分层：简单函数、模块级、系统级
- 多样性：不同语言、不同领域

**数据来源**：
- 开源项目的 Issue + PR
- Stack Overflow 问答
- 编程竞赛题目（验证正确性）
- 合成数据：用强模型生成

**难例挖掘**：
- 当前模型失败的 case
- 边界情况（空输入、大文件等）

---

### 评估指标设计

**功能正确性**：
- **Pass@k**：采样 k 次至少一次通过测试
- **HumanEval / MBPP**：标准 benchmark

**代码质量**：
- 可读性（命名、注释）
- 复杂度（圈复杂度）
- 最佳实践遵循

**任务完成度**：
- 端到端成功率
- 迭代次数（越少越好）
- 工具调用效率

**人工评估**：
- 真实用户满意度
- A/B 测试

---

### 6a. Agent 的长记忆和短记忆分别怎么设计

**短记忆（Working Memory）**：
- 当前对话上下文
- 实现：直接放在 context window
- 管理：滑动窗口、摘要压缩

**长记忆（Long-term Memory）**：

1. **向量数据库存储**
   - 历史对话 embedding
   - 代码库知识
   - 检索相关内容注入 context

2. **结构化存储**
   - 用户偏好（代码风格、常用库）
   - 项目 schema（文件结构、API 定义）
   - key-value 存储

3. **记忆更新策略**
   - 显式保存（用户说"记住这个"）
   - 隐式学习（观察用户修改模式）
   - 定期压缩/遗忘

**代码 Agent 特有**：
- **代码库索引**：AST 解析，函数/类定义索引
- **执行历史**：成功/失败的命令记录
- **项目配置**：依赖、环境变量等

---

## 7. 数据配比如何考虑，有没有其他配比尝试

### 核心考虑因素

1. **任务覆盖**：确保各类任务都有足够数据
   - 代码、数学、写作、对话、推理...

2. **质量 vs 数量**：高质量数据的权重应该更高

3. **语言配比**：中英文比例，根据目标市场调整

4. **难度分层**：简单/中等/困难任务的分布

### 常见配比策略

**方法 1：按比例混合**
```
代码 30% | 数学 15% | 通用对话 25% | 写作 15% | 推理 15%
```

**方法 2：能力分数加权**
- 在验证集上评估各能力
- 能力弱的领域增加数据比例

**方法 3：课程学习（Curriculum Learning）**
- 先简单后困难
- 逐步增加复杂任务比例

**方法 4：动态调整**
- 训练过程中监控各能力变化
- 自动调整配比（如 DoReMi）

### 其他配比尝试

- **重复高质量数据**：优质数据多 epoch
- **去重**：避免数据污染和过拟合
- **合成数据混入**：用强模型生成数据
- **多样性采样**：确保长尾场景覆盖
- **反比例采样**：稀有类别上采样

### 实验发现

- 代码数据能提升推理能力
- 数学数据的质量比数量重要
- 混入少量负例可能有帮助

---

## 8. KL 散度惩罚是什么？为什么需要用这个

### 什么是 KL 散度

**KL 散度**（Kullback-Leibler Divergence）衡量两个概率分布的差异：

```
KL(P || Q) = Σ P(x) log(P(x) / Q(x))
```

在 RLHF 中：
```
KL(π || π_ref) = E_x~π [log π(x) - log π_ref(x)]
```

### 为什么需要 KL 惩罚

**核心目的**：防止策略偏离 reference model（SFT 模型）太远

**具体原因**：

1. **防止 Reward Hacking**
   - RM 不完美，策略可能钻空子
   - KL 约束让策略保持在已知安全区域

2. **保持语言流畅性**
   - SFT 模型已经学会流畅对话
   - 偏离太远可能导致输出质量下降

3. **避免模式崩塌（Mode Collapse）**
   - 防止策略收敛到单一高分模式
   - 保持输出多样性

4. **训练稳定性**
   - 限制更新幅度，防止策略剧烈变化

### 实际使用

```python
# PPO 中的 reward 修正
reward_with_kl = reward - beta * KL(policy || reference)

# beta 的选择
# - 太大：策略几乎不更新，学不到东西
# - 太小：可能 reward hacking，输出变差
# - 通常 0.01 ~ 0.1，需要调参
```

### 替代方案

- **Clip 机制（PPO）**：直接限制概率比的范围
- **Trust Region（TRPO）**：硬约束 KL < δ
- **DPO**：隐式 KL 约束，不需要显式计算

---

## 9. 你觉得 RLHF 最大的问题是什么？

> 这是一道开放题，可以从多个角度回答：

### 角度 1：Reward Model 的局限性

**核心观点**：RM 是人类偏好的不完美代理

- RM 只学到了偏好的「形式」而非「本质」
- 标注者之间存在分歧，RM 学到的是平均偏好
- RM 外推能力差，OOD 区域不可靠
- 导致 Reward Hacking

### 角度 2：人类偏好本身的问题

- 人类偏好不一致、会变化
- 标注者有偏见（偏好长回答、正式语气）
- 复杂任务难以评估（数学推理正确性）
- 「偏好」不等于「有用」

### 角度 3：训练效率和成本

- PPO 需要 4 个模型，显存需求高
- 超参数敏感，调参困难
- 训练不稳定，容易 reward hacking 或模式崩塌

### 角度 4：可扩展性问题（Scalable Oversight）

- 超人模型如何对齐？人类无法提供正确反馈
- AI 辅助评估的可靠性
- Constitutional AI 等方向的探索

### 角度 5：Objective 的定义

- 什么是「好」的回答？
- Helpful、Harmless、Honest 三者有时冲突
- 单一 reward signal 可能过于简化

**好的回答应该**：选择 1-2 个角度深入展开，结合具体例子，并提出自己对解决方案的思考。

---

## 10. 最近有读什么 paper，介绍一下

> 这是考察候选人学习能力和技术热情的问题。

### 评估维度

- **是否真的读了**：能否讲清楚核心贡献
- **理解深度**：能否分析优缺点
- **批判性思维**：有没有自己的看法
- **与工作的关联**：能否联系实际

### 推荐候选人可能会提到的 Paper（2024-2025 热门）

**RLHF 相关**：
- **DPO**：直接偏好优化，跳过 RM
- **DeepSeek-R1**：GRPO + 长思考链
- **Constitutional AI**：AI 自我监督

**模型架构**：
- **Mamba**：State Space Model，线性复杂度
- **MoE 相关**：Mixtral, DeepSeek-V2
- **Ring Attention**：超长上下文

**推理/Agent**：
- **Chain-of-Thought**、**Tree of Thoughts**
- **Self-consistency**
- **ReAct**、**Reflexion**

**合成数据**：
- **Phi 系列**：小模型 + 高质量合成数据
- **Orca**：模仿 GPT-4 推理过程

### 期望的回答结构

```
1. Paper 名称和出处
2. 要解决什么问题
3. 核心方法/贡献
4. 实验结果
5. 我觉得的优点/局限
6. 对我工作的启发
```

### 红旗（不好的回答）

- 只记得名字，说不清内容
- 照搬 abstract，没有自己理解
- 没有关注近期工作（只提 2022 前的经典）

---

## 面试建议

1. **追问方向**：每道题都可以深入追问，比如"PPO 的 clip ratio 怎么选？"、"DPO 的 β 参数影响是什么？"

2. **看思维过程**：比起标准答案，更重要的是候选人能否清晰地组织思路、分析优缺点

3. **结合实践**：可以追问"你实际用过哪个？遇到什么问题？"

4. **第 9、10 题是开放题**：没有标准答案，主要看候选人的思考深度和学习热情