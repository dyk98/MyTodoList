# 场景优化专项整体方案

## 一、背景

我们有两个独立但相关的优化方向：

1. **场景配置自优化**：通过评测、归因、自动生成新配置来提升 AI 对话质量
2. **教育场景并发提升**：通过四象限分类、模型路由、Prompt 压缩来提升系统性能

两者优化的是不同的"层"，但存在共同点：
- 都需要数据驱动
- 都需要闭环验证
- 都可能相互影响（性能优化可能影响质量，质量优化可能影响性能）

**核心问题**：能否将两者整合成一个统一的优化框架？

---

## 二、整合思路

### 2.1 核心洞察

两个系统可以共享同一套数据基础设施和闭环机制，但保持独立的触发和执行逻辑。

```
输入独立 → 分析统一 → 执行独立
```

### 2.2 两个数据源

| 数据源 | 来源 | 驱动的优化 |
|--------|------|-----------|
| 用户输入 | 点赞/踩、对话反馈、评测结果 | 质量优化（配置层） |
| 数据输入 | Token 消耗、延迟、调用频率 | 性能优化（资源层） |

### 2.3 设计原则

1. **数据层统一**：一次采集，多处使用
2. **子系统独立**：各自触发、各自执行、各自验证
3. **关键节点人工决策**：质量与性能冲突时人工介入
4. **渐进式融合**：先独立验证价值，再考虑深度整合

---

## 三、整体架构

```
┌─────────────────────────────────────────────────────────────┐
│              场景优化专项 - 整体架构                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐    │
│  │                 统一数据层 (Day 1 建设)              │    │
│  │  · 统一数据模型                                      │    │
│  │  · 统一采集 SDK                                      │    │
│  │  · 统一存储                                          │    │
│  └─────────────────────────────────────────────────────┘    │
│                            │                                │
│              ┌─────────────┴─────────────┐                  │
│              ▼                           ▼                  │
│  ┌─────────────────────┐    ┌─────────────────────┐        │
│  │   质量优化子系统     │    │   性能优化子系统     │        │
│  │                     │    │                     │        │
│  │ · 评测集管理        │    │ · 四象限分类        │        │
│  │ · 自动归因          │    │ · 路由策略          │        │
│  │ · 配置优化          │    │ · Prompt 压缩       │        │
│  │ · 劣化检测          │    │ · 性能监控          │        │
│  └──────────┬──────────┘    └──────────┬──────────┘        │
│             │                          │                    │
│             └────────────┬─────────────┘                    │
│                          ▼                                  │
│  ┌─────────────────────────────────────────────────────┐    │
│  │              协调层 (Phase 2 建设)                   │    │
│  │  · 冲突检测                                          │    │
│  │  · 人工决策点                                        │    │
│  │  · 联合归因                                          │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 四、统一数据层设计

### 4.1 数据模型

```
会话 (Session)
├── session_id
├── scene_id
├── user_id
├── created_at
│
├── 质量维度
│   ├── user_feedback (点赞/踩/原因)
│   ├── eval_result (评测通过/失败)
│   └── badcase_reason (失败原因分类)
│
├── 性能维度
│   ├── input_tokens / output_tokens
│   ├── system_prompt_tokens
│   ├── response_latency
│   ├── model_used
│   └── quadrant (Q1/Q2/Q3/Q4)
│
└── 行为维度
    ├── dialogue_rounds
    ├── tool_calls []
    │   ├── tool_name
    │   ├── execution_time
    │   └── return_size
    └── scene_switches []
```

### 4.2 统一数据层的价值

| 价值 | 说明 |
|------|------|
| 关联分析 | 发现"压缩 Prompt 后质量下降的 case 集中在哪类场景" |
| 根因定位 | 延迟高是因为模型慢、Tool 慢、还是轮次多 |
| 效果验证 | 同一份数据验证质量和性能两个维度的变化 |
| 避免重复建设 | 一次采集，多处使用 |
| **独立测试能力** | 不触发优化也能作为单独的测试工具，查看各项指标 |

### 4.3 独立测试模式

**重要**：统一数据层和评测能力可以独立于优化流程使用。

```
┌───────────────────────────────────────────────────┐
│           测试模式 vs 优化模式                      │
├───────────────────────────────────────────────────┤
│                                                   │
│  【测试模式】                                      │
│   · 运行评测集，收集各项指标                       │
│   · 生成测试报告（质量、性能、行为数据）            │
│   · 不触发任何自动优化                            │
│   · 用于：版本验收、回归测试、基线建立              │
│                                                   │
│  【优化模式】                                      │
│   · 基于测试结果触发归因和优化                     │
│   · 自动生成优化建议或新配置                       │
│   · 进入优化闭环（自提问→自执行→自归因→自优化）    │
│   · 用于：主动改进、问题修复、持续迭代              │
│                                                   │
└───────────────────────────────────────────────────┘
```

**测试模式的典型使用场景**：

| 场景 | 说明 |
|------|------|
| 新版本上线前验收 | 运行完整评测集，对比新旧版本各项指标 |
| 日常回归测试 | 定期执行，确保系统稳定性 |
| 基线建立 | 首次接入时，建立各项指标的基线数据 |
| 问题诊断 | 用户反馈后，通过测试复现并定位问题 |
| A/B 测试效果对比 | 对比不同配置/策略的实际表现 |

**核心理念**：验证与优化同等重要。系统首先是一个**可信的测试工具**，其次才是一个**自动优化引擎**。

### 4.4 采集清单

#### Token 消耗
| 数据项 | 说明 | 采集频率 |
|--------|------|----------|
| 每轮输入 tokens | 每次用户输入的 token 数量 | 每轮 |
| 每轮输出 tokens | 每次 AI 响应的 token 数量 | 每轮 |
| System Prompt tokens | System Prompt 的长度 | 每场景 |
| User Context tokens | 用户上下文历史的长度 | 每轮 |
| Tool 定义 tokens | Tool 定义的总长度 | 每场景 |

#### 调用频率
| 数据项 | 说明 | 采集频率 |
|--------|------|----------|
| 场景调用次数 | 每个 scene_id 的调用总数 | 每次会话 |
| 时间分布 | 按小时/天统计的调用量 | 每次会话 |
| QPS 占比 | 该场景占总 QPS 的百分比 | 聚合计算 |

#### 对话行为
| 数据项 | 说明 | 采集频率 |
|--------|------|----------|
| 每个场景的对话轮次 | 单个场景内的轮次数 | 每场景 |
| 场景切换序列 | 会话中经历的场景顺序 | 每次会话 |
| 用户发言次数 | 用户主动输入的次数 | 每次会话 |
| AI 回复次数 | AI 响应的次数 | 每次会话 |

#### 响应性能
| 数据项 | 说明 | 采集频率 |
|--------|------|----------|
| 每轮响应延迟 | 单次请求的响应时间 | 每轮 |
| P50/P95/P99 延迟 | 延迟的百分位数 | 聚合计算 |
| 总会话时长 | 从开始到结束的总时间 | 每次会话 |
| 首次响应延迟 | 用户首次输入到首次响应 | 每次会话 |

#### Tool 使用
| 数据项 | 说明 | 采集频率 |
|--------|------|----------|
| Tool 调用次数 | 每个场景调用 Tool 的总次数 | 每场景 |
| Tool 名称 | 具体调用了哪个 Tool | 每次调用 |
| Tool 执行时间 | Tool 执行的耗时 | 每次调用 |
| Tool 返回数据大小 | 返回结果的字节数/tokens | 每次调用 |

#### 质量指标
| 数据项 | 说明 | 采集频率 |
|--------|------|----------|
| 用户满意度评分 | 1-5 分评分 | 每次会话（可选） |
| 错误率和错误类型 | 错误分类统计 | 每轮 |
| 是否需要重试 | 用户是否要求重新生成 | 每轮 |
| 评测通过/失败 | 自动评测结果 | 每次评测 |
| 失败原因分类 | badcase 的具体原因 | 每次评测 |

---

## 五、质量优化子系统

### 5.1 两种优化模式

| 模式 | 适用场景 | 人工介入程度 | 自动化程度 |
|------|---------|-------------|-----------|
| **快速启动模式** | 新场景冷启动、快速验证 | 高（每轮审核标注） | 低（辅助生成） |
| **自动循环模式** | 已有评测集、大规模优化 | 低（最终选择） | 高（自动迭代） |

### 5.2 快速启动模式（推荐新场景使用）

**核心思路**：用户主导 + AI 辅助，通过人工标注驱动迭代优化

```
┌─────────────────────────────────────────────────────────────┐
│                    快速启动优化流程                          │
└─────────────────────────────────────────────────────────────┘
                            │
            1. AI 自动生成初始场景配置
                            │
                            ▼
            ┌──────────────────────────────┐
            │  用户勾选参与模拟的角色人设   │
            │  □ 6岁初学者                │
            │  ☑ 12岁进阶者               │
            │  ☑ 捣蛋学生                 │
            └──────────────────────────────┘
                            │
            2. 按角色逐一生成完整对话轨迹
                            │
                            ▼
            ┌──────────────────────────────┐
            │   对话轨迹 1（12岁进阶者）    │
            │   [1] 学生: 这个怎么做？      │
            │       AI: xxx                │
            │   [2] 学生: 为什么？          │
            │       AI: xxx                │
            │   [3] ...                    │
            │                              │
            │   ✅ 整体评价: 良好           │
            │   ⚠️  第2轮标注: 解释不够深入 │
            └──────────────────────────────┘
                            │
            3. 用户标注评价（语义化）
               - 整体评价：满意/不满意/需改进
               - 单轮标注：指出具体问题
                            │
                            ▼
            4. AI 根据标注自动生成优化配置
                            │
                            ▼
            5. 用新配置重新生成对话轨迹
                            │
                            ▼
            6. 用户审核，满意则结束，否则重复步骤 3-5
```

**关键特点**：
- **零成本启动**：不需要预先准备评测集，AI 自动生成场景配置和对话
- **实时反馈**：用户直接在对话轨迹上标注，无需编写测试用例
- **渐进优化**：每轮只优化标注的问题点，避免过度优化
- **灵活控制**：用户随时可以停止，选择满意的版本

**用户操作界面示意**：

```
┌──────────────────────────────────────────────────────────────┐
│  场景配置快速生成                                             │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  场景名称: [教育场景-简单修改]                                │
│  场景描述: [帮助学生学习编程基础]                             │
│                                                              │
│  [生成初始配置]                                               │
│                                                              │
│  选择测试角色:                                                │
│  ☑ 6岁初学者   ☑ 12岁进阶者   ☑ 捣蛋学生   □ 自定义...       │
│                                                              │
│  [生成对话轨迹]                                               │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│  对话轨迹预览 - 12岁进阶者（第1轮）                            │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  [1] 👦 学生: 老师，这个循环怎么写？                           │
│      🤖 AI: 循环是重复执行代码的方式，我们来看一个例子...      │
│                                                              │
│      标注: [满意 ▼] 备注: _________________________          │
│                                                              │
│  [2] 👦 学生: 为什么要用 i++ 而不是 i+1？                      │
│      🤖 AI: i++ 是简写形式，和 i=i+1 效果相同。               │
│                                                              │
│      标注: [需改进 ▼] 备注: [解释太简单，应该说明区别]       │
│                                        ▲ 用户标注的问题      │
│  [3] 👦 学生: 好的，我试试看                                  │
│      🤖 AI: 太棒了！有问题随时问我。                          │
│                                                              │
│      标注: [满意 ▼] 备注: _________________________          │
│                                                              │
│  整体评价: ○ 满意  ● 需改进  ○ 不满意                        │
│  整体反馈: [第2轮解释不够深入，应该举例说明 i++ 和 i+1 的实际差异] │
│                                                              │
│  [提交并优化配置]  [跳过此角色]  [重新生成]                   │
└──────────────────────────────────────────────────────────────┘
```

### 5.3 自动循环模式（适合已有评测集）

**核心思路**：基于预定义评测集自动迭代优化

```
自提问 → 自执行 → 自归因 → 自优化
   ↑                          ↓
   └──────── 循环 ─────────────┘
```

### 5.4 核心能力

| 能力 | 说明 |
|------|------|
| 角色扮演配置 | 预定义多种测评人设（6岁初学者/12岁进阶者/捣蛋学生等） |
| 评测集管理 | AI 基于角色生成 + 种子扩展 + 人工编辑 |
| 自动评测 | 批量执行评测集，收集结果 |
| 归因分析 | 分析失败原因，定位到配置字段 |
| 配置优化 | 自动生成新配置版本 |
| 循环优化 | 多轮迭代直到达到目标 |
| 劣化检测 | 确保优化不引入新问题 |

### 5.5 角色扮演配置（测评人设）

**为什么需要测评人设**：评测质量依赖于输入的多样性。通过定义不同用户角色，可以模拟真实场景中的各类用户。

**典型角色示例**：

| 角色 | 人设 | 典型行为 | 用途 |
|------|------|---------|------|
| 6岁初学者 | 词汇量有限，注意力易分散，需要鼓励 | "老师这是什么意思"、"太难了不想学了" | 测试引导能力、耐心程度 |
| 12岁进阶者 | 有一定基础，求知欲强，会追问 | "为什么要这样做"、"还有其他方法吗" | 测试深度解释能力 |
| 捣蛋学生 | 故意输入无关内容，测试边界 | "我不想学习"、"给我讲个笑话" | 测试引导回归能力 |
| 自定义角色 | 根据场景定制 | - | 特定场景测试 |

**数据结构**：

```typescript
interface RolePersona {
  id: string;                    // 如 "beginner_6yo"
  name: string;                  // 如 "6岁初学者"
  personality: string;           // 角色人设描述
  characteristics: string[];     // 行为特征
  typicalInputPatterns: string[]; // 典型输入示例
}
```

**使用方式**：
- AI 生成评测集时，基于角色人设生成符合角色特征的对话输入
- 支持角色切换策略：用不同角色改写同一个测试用例，扩展评测覆盖面

### 5.6 触发条件

- 用户反馈（点踩）
- 评测通过率下降
- 定期巡检发现问题
- 人工主动触发

---

## 六、性能优化子系统

### 6.1 四象限分类

```
高资源消耗
     ↑
┌────────────┼────────────┐
│            │            │
│  象限 II   │  象限 I    │
│  低频高耗  │  高频高耗  │
│            │            │
低频 ────────┼──────────── 高频
│            │            │
│  象限 III  │  象限 IV   │
│  低频低耗  │  高频低耗  │
│            │            │
└────────────┼────────────┘
     ↓
低资源消耗
```

### 6.2 优化策略

| 象限 | 代表场景 | 模型策略 | 优化重点 |
|------|---------|---------|---------|
| Q1 高频/高耗 | 多 Tool + 长上下文 | 大模型 | 拆分场景、限制轮次、Fallback |
| Q2 低频/高耗 | 复杂创作 | 大模型 | 压缩 Prompt、按需截断 |
| Q3 低频/低耗 | 调研类 | 小模型 | 批处理 + 合并响应 |
| Q4 高频/低耗 | 触发多、单次轻量 | 小模型 | 减少上下文、控制轮次 ≤3 |

### 6.3 触发条件

- QPS 超过阈值
- P95 延迟超标
- 成本预算告警
- 定期策略评估

---

## 七、协调层设计

### 7.1 冲突场景

质量优化和性能优化可能存在冲突：

| 质量优化建议 | 性能优化建议 | 冲突 |
|-------------|-------------|------|
| 增加 Prompt 描述以提升准确性 | 压缩 Prompt 以节省 Token | 冲突 |
| 增加对话轮次以充分引导 | 限制轮次以提升并发 | 冲突 |
| 使用大模型保证质量 | 路由到小模型降低成本 | 冲突 |

### 7.2 人工决策点

```
┌─────────┐    自动    ┌─────────┐
│ 性能优化 │ ─────────▶│ 质量检测 │
│ 建议压缩 │           │ 发现下降 │
└─────────┘           └────┬────┘
                           │
                           ▼ 触发人工决策
                    ┌─────────────┐
                    │  决策面板    │
                    │             │
                    │ 选项A: 接受压缩，质量可接受  │
                    │ 选项B: 放弃压缩，保质量优先  │
                    │ 选项C: 折中方案（AI 生成）   │
                    └─────────────┘
```

### 7.3 自动决策阈值

不是所有冲突都需要人工介入：

| 场景 | 阈值 | 处理方式 |
|------|------|---------|
| 质量下降 < 5% | 可接受 | 自动接受性能优化 |
| 质量下降 5%-10% | 需关注 | 触发人工决策 |
| 质量下降 > 10% | 不可接受 | 自动拒绝性能优化 |

---

## 八、实施计划

### 8.1 落地原则

**核心思想**：渐进式自动化，每一步都有独立价值

| 原则 | 说明 |
|------|------|
| **人工优先** | 人工参与的部分更稳定，过度自动化带来不确定性 |
| **独立价值** | 每个 Phase 完成后都能独立产生价值，不依赖后续 Phase |
| **数据积累** | 前期人工标注为后续自动化打基础 |
| **风险可控** | 自动化链路越长，误差累积越大，需要渐进验证 |

**自动化链路的风险示例**：
```
AI评判（70%准确）→ AI归因（80%准确）→ AI优化（85%准确）
最终准确率 = 0.7 × 0.8 × 0.85 ≈ 47.6% ❌

结论：过早追求全自动化会导致不可靠
```

### 8.2 质量优化落地节奏

#### Phase Q1：快速启动模式（MVP）

**目标**：帮助用户快速生成和优化场景配置

**为什么先做这个**：
- ✅ 人工主导，AI 辅助 → 风险可控
- ✅ 立即产生价值（帮助用户生成配置）
- ✅ 积累真实标注数据 → 为后续自动化打基础
- ✅ 验证核心假设：角色人设是否有效

**最小化范围**：
```
POST /api/scenes/generate         // AI生成初始配置
POST /api/scenes/:id/simulate     // 按角色生成对话轨迹
POST /api/scenes/:id/optimize     // 基于标注优化配置
GET  /api/rolePersonas            // 角色列表
```

**核心流程**：
1. 用户输入场景名称 + 简单描述
2. AI 生成初始配置
3. 用户勾选 2-3 个角色
4. 生成对话轨迹（每角色 1 条）
5. 用户标注：满意/需改进 + 简单反馈
6. AI 生成优化版本
7. 重复 4-6，2-3 轮后完成

**技术栈**：前端表单 + 单个 LLM 调用 + JSON 存储

---

#### Phase Q2：测试模式

**目标**：提供独立的测试验证能力

**为什么是第二步**：
- ✅ 不依赖自动化判断，基于数据采集
- ✅ 立即有独立价值（验收、回归测试）
- ✅ 为 Phase Q3 的自动化积累数据

**最小化范围**：
```
POST /api/eval/datasets/manual    // 人工编写评测集
POST /api/eval/runs               // 执行评测（只收集数据）
GET  /api/eval/runs/:id/report    // 生成测试报告
```

**核心价值**：
- 新版本上线前跑一遍，看各项指标
- 不做自动判断，只展示：Token、延迟、轮次
- 人工看对话质量，标记 pass/fail

---

#### Phase Q3：自动评判 + 归因

**目标**：AI 辅助评判，减少人工工作量

**门槛条件**：
- ✅ Phase Q1 积累了足够的标注数据
- ✅ Phase Q2 的测试框架已就绪

**最小化范围**：
```
POST /api/eval/runs/:id/judge        // AI 自动评判
POST /api/eval/runs/:id/attribution  // AI 归因分析
```

**关键设计**：
- **评判置信度**：AI 给出 pass/fail + 置信度
- **人工复核**：置信度 < 80% 需人工复核
- **准确率监控**：持续对比自动 vs 人工

**降级方案**：如果准确率不够，回退到「半自动模式」
```json
{
  "autoJudgement": { "pass": false, "confidence": 0.6 },
  "humanReview": { "required": true }
}
```

---

#### Phase Q4：自动优化 + 循环

**目标**：完整的自动化优化闭环

**门槛条件**：
- ✅ Phase Q3 自动评判准确率 > 85%
- ✅ 归因分析有明确价值（用户认可）
- ✅ 有明确的循环优化需求

**最小化范围**：
```
POST /api/eval/runs/:id/optimize   // 生成优化配置
POST /api/eval/loops               // 启动循环优化
POST /api/eval/regression/check    // 劣化检测
```

---

### 8.3 性能优化落地节奏

#### Phase P1：数据采集 + 看板

**目标**：建立性能数据基线

**最小化范围**：
- 采集 SDK（Token/延迟/频率）
- 基础看板（场景维度聚合）
- 四象限自动分类（基于数据）

**核心价值**：
- 知道哪些场景是「高频高耗」
- 为后续优化提供数据支撑

---

#### Phase P2：手动优化 + 验证

**目标**：人工制定优化策略，系统辅助验证

**最小化范围**：
- 人工配置路由规则（场景 → 模型）
- 人工配置压缩策略
- A/B 测试验证效果

**为什么不直接自动优化**：
- 性能优化影响面大，需要人工把控
- 不同场景的优化策略差异大
- 先验证「人工优化」有效，再考虑自动化

---

#### Phase P3：策略推荐

**目标**：AI 推荐优化策略，人工审核执行

**最小化范围**：
- 基于数据生成优化建议
- 预估优化效果（Token 节省、延迟变化）
- 人工审核后执行

---

#### Phase P4：自动优化 + 质量约束

**目标**：自动执行优化，质量约束兜底

**门槛条件**：
- ✅ Phase P2/P3 的优化策略验证有效
- ✅ 质量检测能力就绪（Phase Q2/Q3）

**最小化范围**：
- 自动执行优化策略
- 质量检测兜底（下降 > 10% 自动回滚）

---

### 8.4 整体节奏总览

```
                    质量优化                    性能优化
                       │                          │
    ┌──────────────────┼──────────────────────────┼─────────────────┐
    │                  │                          │                 │
    │   Phase Q1       │                   Phase P1                 │
    │   快速启动模式    │                   数据采集+看板             │
    │   (MVP, 2-3周)   │                   (1-2周)                  │
    │        ↓         │                      ↓                     │
    │   Phase Q2       │                   Phase P2                 │
    │   测试模式        │                   手动优化+验证             │
    │   (1-2周)        │                   (2-3周)                  │
    │        ↓         │                      ↓                     │
    │   Phase Q3       │                   Phase P3                 │
    │   自动评判+归因   │ ←── 数据共享 ───→  策略推荐                 │
    │   (2-3周)        │                   (2-3周)                  │
    │        ↓         │                      ↓                     │
    │   Phase Q4       │                   Phase P4                 │
    │   自动优化+循环   │ ←── 质量约束 ───→  自动优化                 │
    │   (3-4周)        │                   (2-3周)                  │
    │                  │                          │                 │
    └──────────────────┴──────────────────────────┴─────────────────┘
                              │
                              ▼
                        协调层（按需）
                    质量 vs 性能 冲突处理
```

### 8.5 各阶段价值独立性

| Phase | 独立价值 | 技术风险 | 用户可用 |
|-------|---------|---------|---------|
| Q1 快速启动 | ✅ 高 | ✅ 低 | ✅ 立即 |
| Q2 测试模式 | ✅ 高 | ✅ 低 | ✅ 立即 |
| Q3 自动评判 | ⚠️ 中 | ⚠️ 中 | ⚠️ 需验证 |
| Q4 自动优化 | ⚠️ 中 | ❌ 高 | ⚠️ 需打磨 |
| P1 数据看板 | ✅ 高 | ✅ 低 | ✅ 立即 |
| P2 手动优化 | ✅ 高 | ✅ 低 | ✅ 立即 |
| P3 策略推荐 | ⚠️ 中 | ⚠️ 中 | ⚠️ 需验证 |
| P4 自动优化 | ⚠️ 中 | ⚠️ 中 | ⚠️ 需质量兜底 |

### 8.6 风险缓解策略

| 风险 | 缓解措施 | 监控指标 |
|------|---------|---------|
| AI 生成配置质量不稳定 | Phase Q1 用户必须审核 | 重新生成次数 |
| AI 评判不准确 | Phase Q3 人工复核机制 | 自动 vs 人工一致率 |
| 自动优化偏离原意 | Phase Q4 劣化检测 + 人工选择 | 人工拒绝版本比例 |
| 性能优化影响质量 | Phase P4 质量约束兜底 | 质量下降触发回滚次数 |

---

## 九、预期收益

### 效率提升

| 指标 | 现状 | 目标 |
|------|------|------|
| 单场景优化周期 | 1-2 周 | 1-2 天 |
| 问题发现时间 | 用户反馈后 | 主动发现 |
| 性能调优周期 | 人工分析 | 数据驱动 |

### 质量保障

- 每次优化都有评测验证
- 劣化检测确保不引入新问题
- 质量与性能双重守护

### 成本优化

- 四象限分类实现精细化资源调度
- Prompt 压缩降低 Token 消耗
- 模型路由降低大模型使用比例

---

## 十、总结

这个方案的核心理念是：

> **验证与优化同等重要，数据统一是基础，子系统独立是务实，协调层是未来**

系统的双重定位：
1. **测试工具**：独立运行评测，生成质量/性能/行为报告，用于验收、回归、诊断
2. **优化引擎**：基于测试结果驱动自动归因和配置优化

通过统一的数据层，我们可以同时支撑质量优化和性能优化两个子系统，让它们各自独立运行、独立验证价值。在关键冲突点引入人工决策，确保在追求效率的同时不牺牲质量。

最终目标是实现：
- **可信验证**：任何变更都能通过测试量化影响
- **用户反馈** 能快速转化为质量提升
- **性能数据** 能自动驱动资源优化
- **两者协调** 在人工把关下达到全局最优
