## 2025-12-08 深度学习与 RLHF 算法学习

### 1. 学习率相关超参数

- **lr (Learning Rate)**: 控制模型参数更新的步长大小，典型值 0.001, 0.0001
- **min-lr (Minimum Learning Rate)**: 学习率衰减策略中的下限值
  - 常见于 Cosine Annealing 等衰减策略
  - 防止学习率衰减到 0，保持持续微调能力
  - 典型值为 lr 的 1/10 到 1/100

### 2. GRPO 算法 (Group Relative Policy Optimization)

强化学习算法，用于大语言模型 RLHF 训练，由 DeepSeek 等团队提出。

**核心思想**: 对比学习 + 相对优势估计，不需要单独训练 Value 模型

**算法流程**:
1. 对每个 prompt 生成 N 个不同回答（N=4~8）
2. 计算每个回答的奖励分数
3. 组内归一化得到相对优势：`advantage = (reward - mean) / std`
4. 使用 PPO clip 策略更新模型

**优势**:
- 节省显存（无需 Value 网络）
- 训练效率提升 2-3 倍
- 组内对比天然降低方差

### 3. 标准差 (Standard Deviation)

**定义**: 衡量数据离散程度的统计量，表示数据偏离平均值的程度

**计算公式**:
```
std = √(Σ(x - μ)² / n)
```

**含义**:
- 标准差越小 → 数据越集中、越稳定
- 标准差越大 → 数据越分散、波动越大

**在 GRPO 中的应用**: 用于归一化不同 prompt 的奖励值，使其可比较

### 4. PPO Clip 参数

**作用**: 限制策略更新的幅度，防止训练崩溃

**机制**:
```javascript
const clippedRatio = clip(ratio, 1 - epsilon, 1 + epsilon);
```

- 典型值 `epsilon = 0.2`，限制新策略最多变化 ±20%
- 类比：给策略更新加"刹车"

### 5. RLHF 奖励函数策略

**两种方式**:

1. **分阶段训练**（推荐）:
   - 第一阶段用奖励函数 A（如优化有用性）
   - 第二阶段用奖励函数 B（如优化安全性）
   - 目标明确，易于调试

2. **综合奖励函数**:
   - 一次性定义完整奖励，加权组合多个目标
   - 效率高但权重难调

**实际工程**: 常用混合策略，如 SFT → RLHF(有用性+安全性) → RLHF(专项优化)

---

## 2025-12-04 今天学到的核心知识点

1. Loss（损失函数）

衡量模型预测与真实值的差距，越小越好。训练的目标就是降低 Loss。

2. 参数（Parameters）

- 神经网络中的权重值（w, b），由训练自动学习
- 数量：百万到万亿级
- 没有预定义含义，功能是训练过程中"涌现"出来的
- 单个参数完全无法理解，整体是黑盒

3. 调整参数 = 梯度下降

计算 Loss → 反向传播算梯度 → 更新参数
新参数 = 旧参数 - 学习率 × 梯度
自动重复数百万次，让模型越来越准确。

4. 增加参数 = 增强能力

- 方法：增加层数、增加每层神经元数
- 理论上可以任意加，实际受限于算力、数据、成本
- GPT-3: 1750亿参数，训练成本数千万美元

5. 超参数（Hyperparameters）

人类手动设置的训练配置，训练前就固定：
- learning_rate: 参数调整的步长
- batch_size: 每次训练用多少样本
- epochs: 训练多少轮
- hidden_size: 每层多少神经元

6. 推理超参数（采样参数）

控制模型生成文本的方式：
- temperature: 控制随机性（0=确定，2=疯狂）
- top_p: 从累积概率前 p% 的词中选（常用 0.9）
- top_k: 从概率最高的 k 个词中选

代码生成 → temperature=0（要确定性）
创意写作 → temperature=1.2（要多样性）

7. 奖励函数（Reward Function）

强化学习中评价输出好坏的打分系统：
- 好输出 → 高分 → 增加概率
- 坏输出 → 低分 → 降低概率

ChatGPT 的 RLHF 训练：
1. 预训练（学语言）
2. 监督微调（学对话）
3. RLHF（用奖励函数优化成"有用、安全、诚实"）

  ---
关键对比

| 概念    | 谁设置  | 数量     | 例子                  |
  |-------|------|--------|---------------------|
| 参数    | 自动学习 | 数十亿    | w=0.00347           |
| 训练超参数 | 人类设置 | 10-50个 | learning_rate=0.001 |
| 推理超参数 | 用户设置 | 3-5个   | temperature=0.7     |
| 奖励函数  | 人类设计 | 1个     | 有用+10分，有害-100分      |

  ---
核心理解

神经网络 = 黑盒系统
- 通过海量参数自动学习
- 人类只能设置目标（Loss、Reward），无法精确控制内部逻辑
- 参数越多 → 能力越强 → 成本越高 → 越难理解