## 2025-12-29
## 📚 今日学习总结：重点内容

### 1. **Attention 层（自注意力机制）**

* **作用**：Attention 层是 Transformer 架构的核心，允许模型在生成每个 token 时，不仅仅依赖当前输入，还能“关注”全局上下文，理解词与词之间的关系。
* **基本原理**：通过计算每个词的 **Query (Q)**、**Key (K)** 和 **Value (V)**，并用这些向量计算出注意力权重，决定哪些信息是最相关的。

    * **Q**：表示当前词想要找到什么信息；
    * **K**：表示每个词能提供什么信息；
    * **V**：是每个词的实际内容。
* **计算流程**：

    1. **Q** 和 **K** 做点积，计算相似度；
    2. 使用 **softmax** 将相似度转化为权重；
    3. 将这些权重加权应用到 **V** 向量，生成最终的上下文表示。
* **Multi-Head Attention**：不仅使用一个注意力头，而是并行使用多个头，分别从不同角度关注不同信息，提高模型的表达能力。

### 2. **LLM 中的推理与 token 消耗**

* **模型推理过程**：

    * 在 **LLM** 中，每个生成的 token 都依赖 **self-attention**，并计算所有历史 token 的关系。
    * 即使生成过程快速，模型依然在每个 token 的生成中读入整个上下文，而不是仅依赖最近的几轮信息。
* **token 消耗**：

    * 每次生成的输出和上下文的输入都会消耗 **input token**，而不只是模型的推理过程。因此，即便推理过程中模型的计算资源消耗较低，输入 token 的增加会导致较高的计费。
* **计算优化**：

    * **KV 缓存**：通过缓存每个 token 的 Key 和 Value，避免重复计算，优化了计算过程。
        *   **滑动窗口（Sliding Window Attention）** 和 **稀疏注意力（Sparse Attention）**：这些机制使得模型在长上下文中，只关注当前窗口或重要位置的 token，从而减少计算量和提高效率。
    
    ### 3. **GRPO vs GSPO (强化学习优化算法)**
    
    * **共同点**:
        * 都属于 **Rloo** 家族变体，核心目的是**移除 Critic 模型（价值网络）**以节省显存。
        * 都通过**分组采样 (Group Sampling)** 和计算**组内相对优势**来替代 Value Function。
    * **GRPO (DeepSeek-R1/Math)**:
        * **优化粒度**: **Token 级**。
        * **机制**: 将序列级的奖励分配给每个 Token 进行更新。
        * **缺点**: 信用分配错位导致**梯度方差极大**。在训练 **MoE** 模型时极不稳定，需要 **Routing Replay** 等补丁。
    * **GSPO (Qwen3)**:
        * **优化粒度**: **Sequence 级**。
        * **机制**: 直接计算**整个序列**的新旧概率比，进行截断和优化。
        * **优点**: **低方差**，训练更稳定，**原生支持 MoE** 且无需额外 Trick。被视为 GRPO 的进阶版。
    
    ### 4. **MoE (混合专家模型)**
    
    * **结构**: 将前馈网络 (FFN) 切分为多个 **Experts (专家)**，引入 **Router (路由器)** 动态分发 Token。
    * **特性**: **稀疏激活**。对于每个 Token，仅激活一小部分参数（如 Top-K 个专家）。
    * **参数误区**: `专家总数 ≠ 总参数量 / 激活参数量`。
        * 原因：Attention 层、Embedding 层等是**共享参数 (Dense)**，始终被激活且占比很大。
    
    ### 5. **Routing Replay (路由回放)**
    
    * **背景**: GRPO 训练 MoE 时，参数更新会导致 Router 决策变化（"路由漂移"），使旧策略概率计算失效，引发训练崩溃。
    * **机制**: 在计算旧策略 (Reference Policy) 概率时，**强制复用**采样阶段记录的专家路由路径。
    * **作用**: 消除路由变化带来的噪声，稳定 MoE 的 RL 训练。


## 2025-12-24

数据规模与步数

- data_file_num_lines = [2048, 6973]：原始数据行数
- drop-last 到 ppo_rollout_global_batch_size=32 的倍数：2048 + 6944 = 8992
- --ppo-step-per-epoch -1 + --ppo-auto-calc-args → 自动计算
  ppo_step_per_epoch = 8992 / 32 = 281

采样与总量

- --ppo-max-epochs 1：PPO 外层 epoch 数 = 1
- --ppo-rollout-global-batch-size 32：每个 PPO step 的 prompt 数（全局）
- --ppo-rollout-micro-batch-size 1：rollout 时每个 DP 的微批大小
- --ppo-sampling-repeat 8：每个 prompt 采样 8 次
  → 每 step 生成样本数 = 32 * 8 = 256
- --ppo-sampling-keep 8 + --ppo-sampling-keeping-strategy all：全部保留
  → 每 step 用于训练的样本数 = 256
- 总生成样本数：1 * 281 * 32 * 8 = 71,936
- 总保留样本数：同上 71,936

训练迭代（自动计算）

- --global-batch-size 32（actor）
- kept_gbs = 32 * 8 = 256
- --ppo-max-epochs-2 1（rollout 后每批训练 epoch2 次数）
- train_iters_each_rollout = 256 / 32 * 1 = 8
- train_iters = 1 * 281 * 8 = 2248
- --ppo-step-save-interval 5 → save_interval = 5 * 8 = 40

长度相关

- --seq-length 32768：输入+输出的最大总长度
- --ppo-resp-seq-len 28672：生成响应的最大长度
- --ppo-logps-fwd-micro-batch-size 4：logprob 前向的微批大小

## 2025-12-23 训练检查点与 KL 散度

### 1. LOAD_CHECKPOINT_DIR

- **用途**: 加载当前训练模型的检查点
- **场景**:
  - 继续训练：恢复模型权重 + 优化器状态 + 训练步数
  - 微调训练：加载预训练基座模型
- **缺少影响**: 从随机初始化开始，训练成本和时间大幅增加

### 2. REF_LOAD_CHECKPOINT_DIR

- **用途**: 加载参考模型（Reference Model）
- **场景**: RL 训练（RLHF/PPO/GRPO）中提供固定基准
- **作用**: 计算 KL 散度，防止模型偏离太远
- **缺少影响**: 模型过度优化奖励，产生退化输出

### 3. KL 散度惩罚

- **公式**: `Loss = -奖励 + β * KL(训练模型 || 参考模型)`
- **作用**: 防止 Reward Hacking（模型只追求高分，丧失原能力）
- **β 典型值**: 0.01~0.1，越大越保守
- **核心**: REF 模型必须冻结，提供固定基准

### 4. 典型配置

```bash
# RL 训练
LOAD_CHECKPOINT_DIR=/path/to/sft_model       # 会更新
REF_LOAD_CHECKPOINT_DIR=/path/to/sft_model   # 保持不变
```

---

## 2025-12-18 分布式训练并行策略

缩放，Scaling Laws 缩放定律，通过参数量、训练数据量、参数量来实现 scaling，某个节点会出现涌现。缩放定律带来可预测性，模型性能与那三个存在精确的幂律关系
性能 ∝ (参数量)^0.07
成本 ∝ (参数量)^1.3
性能 ∝ (数据量)^0.05

### GPU分组结构

```
┌─────────────────────────────────────────┐
│  总共8张GPU（H20 96GB）                  │
└─────────────────────────────────────────┘
│
├─ TP=2: 分成4组，每组2张卡
│        (卡0-1, 卡2-3, 卡4-5, 卡6-7)
│
├─ CP=4: 4组分别处理序列的不同部分
│        Group0处理tokens[0:8K]
│        Group1处理tokens[8K:16K]
│        Group2处理tokens[16K:24K]
│        Group3处理tokens[24K:32K]
│
├─ PP=1: 所有层在同一个pipeline stage
│
└─ For MoE层：
   EP=4: 4组分别负责不同专家
   ETP=2: 每组内2张卡切分专家权重
```

### 参数含义

分布式训练的并行策略参数，用于将大模型在多 GPU 上拆分计算：

- **TP (Tensor Parallelism)**: 张量并行，将单个模型层的权重矩阵切分到多张卡上并行计算
- **CP (Context Parallelism)**: 上下文并行，将长序列按 token 范围切分到不同组 GPU 处理
- **PP (Pipeline Parallelism)**: 流水线并行，将模型的不同层切分到不同 pipeline stage
- **EP (Expert Parallelism)**: 专家并行，在 MoE（混合专家）模型中将不同专家分配到不同组
- **ETP (Expert Tensor Parallelism)**: 专家张量并行，在单个专家内部切分权重到多张卡

---

## 2025-12-08 深度学习与 RLHF 算法学习

### 1. 学习率相关超参数

- **lr (Learning Rate)**: 控制模型参数更新的步长大小，典型值 0.001, 0.0001
- **min-lr (Minimum Learning Rate)**: 学习率衰减策略中的下限值
  - 常见于 Cosine Annealing 等衰减策略
  - 防止学习率衰减到 0，保持持续微调能力
  - 典型值为 lr 的 1/10 到 1/100

### 2. GRPO 算法 (Group Relative Policy Optimization)

强化学习算法，用于大语言模型 RLHF 训练，由 DeepSeek 等团队提出。

**核心思想**: 对比学习 + 相对优势估计，不需要单独训练 Value 模型

**算法流程**:
1. 对每个 prompt 生成 N 个不同回答（N=4~8）
2. 计算每个回答的奖励分数
3. 组内归一化得到相对优势：`advantage = (reward - mean) / std`
4. 使用 PPO clip 策略更新模型

**优势**:
- 节省显存（无需 Value 网络）
- 训练效率提升 2-3 倍
- 组内对比天然降低方差

### 3. 标准差 (Standard Deviation)

**定义**: 衡量数据离散程度的统计量，表示数据偏离平均值的程度

**计算公式**:
```
std = √(Σ(x - μ)² / n)
```

**含义**:
- 标准差越小 → 数据越集中、越稳定
- 标准差越大 → 数据越分散、波动越大

**在 GRPO 中的应用**: 用于归一化不同 prompt 的奖励值，使其可比较

### 4. PPO Clip 参数

**作用**: 限制策略更新的幅度，防止训练崩溃

**机制**:
```javascript
const clippedRatio = clip(ratio, 1 - epsilon, 1 + epsilon);
```

- 典型值 `epsilon = 0.2`，限制新策略最多变化 ±20%
- 类比：给策略更新加"刹车"

### 5. RLHF 奖励函数策略

**两种方式**:

1. **分阶段训练**（推荐）:
   - 第一阶段用奖励函数 A（如优化有用性）
   - 第二阶段用奖励函数 B（如优化安全性）
   - 目标明确，易于调试

2. **综合奖励函数**:
   - 一次性定义完整奖励，加权组合多个目标
   - 效率高但权重难调

**实际工程**: 常用混合策略，如 SFT → RLHF(有用性+安全性) → RLHF(专项优化)

---

## 2025-12-04 今天学到的核心知识点

1. Loss（损失函数）

衡量模型预测与真实值的差距，越小越好。训练的目标就是降低 Loss。

2. 参数（Parameters）

- 神经网络中的权重值（w, b），由训练自动学习
- 数量：百万到万亿级
- 没有预定义含义，功能是训练过程中"涌现"出来的
- 单个参数完全无法理解，整体是黑盒

3. 调整参数 = 梯度下降

计算 Loss → 反向传播算梯度 → 更新参数
新参数 = 旧参数 - 学习率 × 梯度
自动重复数百万次，让模型越来越准确。

4. 增加参数 = 增强能力

- 方法：增加层数、增加每层神经元数
- 理论上可以任意加，实际受限于算力、数据、成本
- GPT-3: 1750亿参数，训练成本数千万美元

5. 超参数（Hyperparameters）

人类手动设置的训练配置，训练前就固定：
- learning_rate: 参数调整的步长
- batch_size: 每次训练用多少样本
- epochs: 训练多少轮
- hidden_size: 每层多少神经元

6. 推理超参数（采样参数）

控制模型生成文本的方式：
- temperature: 控制随机性（0=确定，2=疯狂）
- top_p: 从累积概率前 p% 的词中选（常用 0.9）
- top_k: 从概率最高的 k 个词中选

代码生成 → temperature=0（要确定性）
创意写作 → temperature=1.2（要多样性）

7. 奖励函数（Reward Function）

强化学习中评价输出好坏的打分系统：
- 好输出 → 高分 → 增加概率
- 坏输出 → 低分 → 降低概率

ChatGPT 的 RLHF 训练：
1. 预训练（学语言）
2. 监督微调（学对话）
3. RLHF（用奖励函数优化成"有用、安全、诚实"）

  ---
关键对比

| 概念    | 谁设置  | 数量     | 例子                  |
  |-------|------|--------|---------------------|
| 参数    | 自动学习 | 数十亿    | w=0.00347           |
| 训练超参数 | 人类设置 | 10-50个 | learning_rate=0.001 |
| 推理超参数 | 用户设置 | 3-5个   | temperature=0.7     |
| 奖励函数  | 人类设计 | 1个     | 有用+10分，有害-100分      |

  ---
核心理解

神经网络 = 黑盒系统
- 通过海量参数自动学习
- 人类只能设置目标（Loss、Reward），无法精确控制内部逻辑
- 参数越多 → 能力越强 → 成本越高 → 越难理解